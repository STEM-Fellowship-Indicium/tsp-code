##
## Adjust to relative path
##
if __name__ == "__main__":
    import sys

    sys.path.append("src")


##
## Imports
##
import torch
import torch.nn as nn
import torch.optim as optim


##
## Generated by ChatGPT
##
class CNN(nn.Module):
    ##
    ## Constructor
    ##
    def __init__(self):
        """Initializes the CNN model"""
        super(CNN, self).__init__()

        ##
        ## Variables
        ##
        self.max_nodes = 100  # Maximum number of nodes in the graph
        self.in_channels = 1  # Number of input channels (assuming the graph is unweighted or weights are normalized)
        self.size = 3200  # Size of the input tensor

        ## Convolutional Layer 1
        ##
        ## Applies a 2D convolution over an input signal (the adjacency matrix) composed of several input planes.
        ## Here, we're starting with 1 input channel (assuming the graph is unweighted or weights are normalized),
        ## expanding to 16 output channels, using a 3x3 kernel, with stride 1 and padding to keep the output size the same.
        self.conv1 = nn.Conv2d(
            self.in_channels, out_channels=16, kernel_size=3, stride=1, padding=1
        )

        ## Convolutional Layer 2
        ##
        ## Takes the 16 channel output from conv1 and applies another convolution, increasing the depth to 32 channels.
        ## Uses the same kernel size, stride, and padding for consistency.
        self.conv2 = nn.Conv2d(
            in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1
        )

        ## ReLU Activation Function
        ##
        ## Applies the rectified linear unit function element-wise: ReLU(x) = max(0, x)
        self.relu = nn.ReLU()

        ## Fully Connected Layer
        ##
        ## Applies a linear transformation to the incoming data: y = xA^T + b
        ## Here, we're taking the 32 channels from the second convolutional layer and flattening them into a 1D tensor.
        ## This is then passed through a fully connected layer to produce the final output.
        self.fc = nn.Linear(self.size, self.max_nodes)

        ##
        ## End of function
        ##

    ##
    ## Forward pass
    ##
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass of the network

        Args:
            x (torch.Tensor): The input tensor

        Returns:
            torch.Tensor: The output tensor
        """
        # Applying the first convolutional layer followed by ReLU activation function
        x = self.relu(self.conv1(x))

        # Applying the second convolutional layer followed by ReLU activation function
        x = self.relu(self.conv2(x))

        # Flattening the output from the second convolutional layer
        x = x.view(-1, self.size)

        # Applying the fully connected layer
        x = self.fc(x)

        # We need to reshape the output to match the expected output shape
        return x.view(self.in_channels, -1)

    ##
    ## End of class
    ##


##
## This tests the cnn class only if we're executing THIS current file.
##
## This is so that if we import the CNN class from another file, this
## code (in the 'if' statement) won't run.
##
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = CNN().to(device)

    ##
    ## Training
    ##
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    batch_size = 1

    # Sample input tensor (x) and target tensor (y)
    x = torch.randn(
        model.in_channels, model.in_channels, model.max_nodes, model.max_nodes
    ).to(device)

    y = torch.randint(low=0, high=100, size=(batch_size,)).to(device)

    # Forward pass and loss calculation
    output = model(x)
    loss = criterion(output, y)

    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print("Training complete")
    print("Loss:", loss.item())
    print("Output:", output)


##
## End of file
##
